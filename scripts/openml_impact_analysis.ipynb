{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCCl0TllgC4A"
   },
   "source": [
    "# OpenML Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "519xCC91_sMH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz5J5Z7bVhhv"
   },
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAzne5nHARM6"
   },
   "outputs": [],
   "source": [
    "# Load survey data\n",
    "\n",
    "path1 = '../data/collected_papers.csv' # Original list of collected papers\n",
    "path2 = \"../data/Final_survey_data.csv\" # Survey Results\n",
    "\n",
    "df = pd.read_csv(path2)\n",
    "\n",
    "print(\"Total no of reviews recieved: \", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYZ9Bi1qEWqk"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhQQfvvt9kH8"
   },
   "source": [
    "### Sanity Check:\n",
    "1. Find dublicates and remove duplicates (matched using Paper ID column).\n",
    "2. Match paper ID and paper title from original list.\n",
    "3. Remove papers with wrong year (<2014) \n",
    "4. Exclude papers from year =2025. \n",
    "5. Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4xo6Knlovwe"
   },
   "outputs": [],
   "source": [
    "Total_papers = pd.read_csv(path1) # Original list of scraped paper\n",
    "print(\"Total number of scraped papers: \", len(Total_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papers which not available or behind paywall\n",
    "unvailable_papers = len(Total_papers) - len(df)\n",
    "print(unvailable_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s0As6Lq9jp3"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary mapping Paper ID to Title from the original list. \n",
    "assigned_papers_dict = Total_papers[[\"Paper ID\", \"Title\"]].set_index('Paper ID').to_dict()\n",
    "assigned_papers_dict = assigned_papers_dict[\"Title\"]\n",
    "assigned_papers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xod_d_tRgC4F"
   },
   "outputs": [],
   "source": [
    "sheet1 = Total_papers[[\"Paper ID\", \"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]]\n",
    "sheet1.rename(columns={\"Paper ID\": \"Paper ID (from shared sheet)\"}, inplace=True)\n",
    "df = df.merge(sheet1, on=\"Paper ID (from shared sheet)\", how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayYUeZPFz3Dd"
   },
   "outputs": [],
   "source": [
    "# Step 1: Remove Dublicates\n",
    "\n",
    "# Duplicate entries\n",
    "duplicate_rows = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
    "\n",
    "print(\"No. of duplicates:\", len(duplicate_rows))\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3tKQM5f0AGk"
   },
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
    "# Save duplicates to a separate DataFrame before removal\n",
    "duplicates_removed = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")]\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")\n",
    "\n",
    "print(\"No of duplicates removed: \", len(duplicates_removed))\n",
    "duplicates_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88YrvITVAKTO"
   },
   "outputs": [],
   "source": [
    "print(\"No of reviews after duplicate removal: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YWfRQuIsSl_l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Remove mismatches\n",
    "# All entries where the paper title and paper ID in the survey does not match the title in the original list\n",
    "# Create a DataFrame to store mismatches\n",
    "\n",
    "mismatches = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        paper_id = row[\"Paper ID (from shared sheet)\"]\n",
    "        paper_title = row[\"Paper Title\"].strip().lower()\n",
    "        assigned_title = assigned_papers_dict.get(paper_id).strip().lower()\n",
    "\n",
    "        if paper_title not in assigned_title:\n",
    "            mismatches.append({\n",
    "                \"Paper ID\": paper_id,\n",
    "                \"Given Title\": row[\"Paper Title\"],\n",
    "                \"Expected Title\": assigned_papers_dict.get(paper_id, \"Not Found\")\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        mismatches.append({\n",
    "            \"Paper ID\": row[\"Paper ID (from shared sheet)\"],\n",
    "            \"Error\": str(e),\n",
    "            \"Given Title\": row.get(\"Paper Title\", \"Not Found\")\n",
    "        })\n",
    "        continue\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "mismatches_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB7cPdcY2EKD"
   },
   "outputs": [],
   "source": [
    "# No Mismatches between the paper title and paper ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJamFvhNzDEU"
   },
   "outputs": [],
   "source": [
    "print(\"No of reviews after mismatch entry removal: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m49r76Sr3EB-"
   },
   "outputs": [],
   "source": [
    "# Step 5: Drop rows with all NaN or NaT values\n",
    "print(len(df))\n",
    "df = df.dropna(how=\"all\")\n",
    "\n",
    "print(\"no of reviews after empty row removal:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmSIfxId3_c2"
   },
   "outputs": [],
   "source": [
    "# Step 3 and 4: Remove papers with a year before 2014 and after 2024.\n",
    "# Paper year before 2014 is an incorrect entry. \n",
    "# Paper year after 2024 are not considered in the survey.\n",
    "\n",
    "# Convert to numeric, invalid strings become NaN\n",
    "df[\"Paper Year\"] = pd.to_numeric(df[\"Paper Year\"], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in 'Paper Year'\n",
    "df = df[df[\"Paper Year\"].notna()]\n",
    "\n",
    "\n",
    "# Convert to integer \n",
    "df[\"Paper Year\"] = df[\"Paper Year\"].astype(int)\n",
    "print(len(df))\n",
    "\n",
    "# # Check the updated column type\n",
    "# print(df[\"Paper Year\"].dtypes)\n",
    "\n",
    "unvailable_papers = unvailable_papers + (1770-len(df))\n",
    "\n",
    "# pepers with NaN or invalid entry type (not int) are unvalible (or being paywall)\n",
    "print(\"unvailable_papers:\", unvailable_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entries with year before 2014 or after 2024\n",
    "\n",
    "out_of_range_2014 = df[(df[\"Paper Year\"] < 2014)]\n",
    "\n",
    "print(\"Papers with year before 2014:\", len(out_of_range_2014))\n",
    "\n",
    "\n",
    "out_of_range_2024 = df[(df[\"Paper Year\"] >2024)]\n",
    "print(\"Papers with year after 2025:\", len(out_of_range_2024))\n",
    "# print(\"Papers with year after 2025:\", out_of_range_2024)\n",
    "\n",
    "no_of_papers_before_out_of_year_removal = len(df)\n",
    "\n",
    "# Remove paper year before 2014 and after 2024\n",
    "df = df[(df[\"Paper Year\"] >= 2014) & (df[\"Paper Year\"] <= 2024)]\n",
    "no_of_papers_after_out_of_year_removal = len(df)\n",
    "\n",
    "print(\"No of papers after removal of papers (before 2014 and after 2025):\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN3bacOo9cfg"
   },
   "outputs": [],
   "source": [
    "print(\"Papers removed (wrong year or after 2024): \", no_of_papers_before_out_of_year_removal - no_of_papers_after_out_of_year_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Paper Available\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1M1RFW5P0q1R"
   },
   "outputs": [],
   "source": [
    "df[\"Paper Available\"] = df[\"Paper Available\"].astype(str).str.lower()\n",
    "\n",
    "count_yes = df[\"Paper Available\"].value_counts().get(\"yes\", 0)\n",
    "print(count_yes)\n",
    "print(len(df)-count_yes)\n",
    "\n",
    "unvailable_papers = unvailable_papers + (len(df)-count_yes)\n",
    "print(\"Total no of unvailable_papers:\", unvailable_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0PWp6MJ1jYJ"
   },
   "outputs": [],
   "source": [
    "df[\"Paper in English\"] = df[\"Paper in English\"].astype(str).str.lower()\n",
    "\n",
    "count_yes = df[\"Paper in English\"].value_counts().get(\"yes\", 0)\n",
    "print(count_yes)\n",
    "print(len(df)-count_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8ZTz_WkgC4I"
   },
   "outputs": [],
   "source": [
    "# No fo paper which are both not (completely) available and not in english\n",
    "overlap = df[(df[\"Paper Available\"] != \"yes\") & (df[\"Paper in English\"] != \"yes\")]\n",
    "print(len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xbi0A9kP628A"
   },
   "outputs": [],
   "source": [
    "# Filter rows not in final_df\n",
    "not_in_final_df = df[~((df[\"Paper Available\"] == \"yes\") &  (df[\"Paper in English\"] == \"yes\"))]\n",
    "\n",
    "print(len(not_in_final_df))\n",
    "# Display the rows not in final_df\n",
    "not_in_final_df[[\"Paper Available\", \"Paper in English\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhEJ8Azj9750"
   },
   "outputs": [],
   "source": [
    "# Should only consider paper available + in english\n",
    "available_papers = df[(df[\"Paper Available\"] == \"yes\") & (df[\"Paper in English\"] == \"yes\")].copy()\n",
    "print(\"Final number of papers for analysis: \", len(available_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all columns of str type to lower case\n",
    "\n",
    "# Identify string columns\n",
    "str_cols = available_papers.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "# print(str_cols)\n",
    "\n",
    "# Convert all string columns to lowercase\n",
    "for col in str_cols:\n",
    "    available_papers[col] = available_papers[col].astype(str).str.lower()\n",
    "    \n",
    "available_papers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq_ceWQpgC4J"
   },
   "source": [
    "Final statistics\n",
    "\n",
    "1. Total exctracted paper: 1786\n",
    "2. Paper used in analysis: 1528\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B30J-apAgC4J"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZbAhJSve9-0"
   },
   "outputs": [],
   "source": [
    "\n",
    "columns_to_analyze = [\"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]\n",
    "\n",
    "# Calculate the percentage of True values for each column\n",
    "percentages = available_papers[columns_to_analyze].mean() * 100\n",
    "\n",
    "# Print the results\n",
    "for column, percentage in percentages.items():\n",
    "    print(f\"Percentage of papers with True for {column}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-9378NBGGRu"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of papers per year\n",
    "papers_per_year = available_papers[\"Paper Year\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(papers_per_year.index, papers_per_year.values, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.title(\"Number of OpenML Cited Papers Per Year\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(linestyle='--', alpha=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCdfjAQJYFR7"
   },
   "outputs": [],
   "source": [
    "# What percentage of the papers does just cite OpenML, but not actively interact with it?\n",
    "\n",
    "interaction_columns = [\n",
    "    \"Does the paper use datasets from OpenML?\",\n",
    "    \"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\",\n",
    "    \"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \",\n",
    "    \"Does the paper upload datasets to OpenML?\",\n",
    "    \"Does the paper upload experiment data to OpenML?\",\n",
    "    \"Does the paper interact in any other way with OpenML?\"\n",
    "]\n",
    "\n",
    "just_citing_papers = available_papers[(available_papers[interaction_columns] == \"no\").all(axis=1)]\n",
    "\n",
    "percentage_citing_only = (len(just_citing_papers) / len(available_papers)) * 100\n",
    "print(len(just_citing_papers), round(percentage_citing_only, 2))\n",
    "\n",
    "core_citing_papers = just_citing_papers[just_citing_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]\n",
    "print(len(core_citing_papers), round((len(core_citing_papers)/len(just_citing_papers))*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdQJYPjIfZIW"
   },
   "outputs": [],
   "source": [
    "# out of above papers which just cites openml, % of paper citing Openml-2014 paper\n",
    "\n",
    "just_citing_openml_2014 = just_citing_papers[just_citing_papers[\"openml-2014\"] == True]\n",
    "print(len(just_citing_openml_2014), (len(just_citing_openml_2014) / len(just_citing_papers)) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi5-nmvVHPVg"
   },
   "outputs": [],
   "source": [
    "english_counts = df[\"Paper in English\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(english_counts, labels=english_counts.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Papers in English\")\n",
    "\n",
    "\n",
    "core_member_counts = available_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"].value_counts()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(core_member_counts, labels=core_member_counts.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Papers with OpenML Core Member Co-author\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3xB5ML_IMPu"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "dataset_papers = available_papers[available_papers[\"Does the paper use datasets from OpenML?\"] == \"yes\"]\n",
    "dataset_percentage = (len(dataset_papers) / len(available_papers)) * 100\n",
    "print(dataset_percentage, (len(dataset_papers)))\n",
    "print(\"Number of paper by core-authors: \",len(dataset_papers[dataset_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "unclear_dataset_papers = available_papers.dropna(subset=[\"If unclear, one sentence explanation?\"])\n",
    "unclear_dataset_papers = unclear_dataset_papers[[\"Does the paper use datasets from OpenML?\", \"If unclear, one sentence explanation?\"]]\n",
    "unclear_dataset_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4VmZNdkKJAC"
   },
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "benchmark_papers = available_papers[available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"yes\"]\n",
    "benchmark_percentage = (len(benchmark_papers) / len(available_papers[available_papers[\"Paper Year\"] >= 2017])) * 100\n",
    "print(benchmark_percentage, len(benchmark_papers))\n",
    "\n",
    "benchmark_datasets = benchmark_papers[\"If yes, which benchmark suites?\"]\n",
    "\n",
    "print(\"Number of paper by core-authors: \",len(benchmark_papers[benchmark_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "# Print list of benchmark datasets\n",
    "unique_benchmark_datasets = (\n",
    "    benchmark_datasets.str.lower().str.strip().unique()\n",
    ")\n",
    "\n",
    "\n",
    "unique_benchmark_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWw_HzTJKbs5"
   },
   "outputs": [],
   "source": [
    "#  Experiment data (Runs)\n",
    "\n",
    "experiment_papers = available_papers[available_papers[\"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \"] == \"yes\"]\n",
    "experiment_percentage = (len(experiment_papers) / len(available_papers)) * 100\n",
    "\n",
    "print(len(experiment_papers) ,experiment_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(experiment_papers[experiment_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "\n",
    "experiment_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: how does it use this data?\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV6nngGINXF7"
   },
   "outputs": [],
   "source": [
    "upload_datasets = available_papers[available_papers[\"Does the paper upload datasets to OpenML?\"] == \"yes\"]\n",
    "\n",
    "upload_datasets_percentage = len(upload_datasets) / len(available_papers) * 100\n",
    "\n",
    "print( len(upload_datasets), upload_datasets_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(upload_datasets[upload_datasets[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "\n",
    "upload_datasets[[\"Paper ID (from shared sheet)\", \"Does the paper upload datasets to OpenML?\", \"If yes, which dataset?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCvmqWoROv6y"
   },
   "outputs": [],
   "source": [
    "upload_experiment_data_papers = available_papers[available_papers[\"Does the paper upload experiment data to OpenML?\"] == \"yes\"]\n",
    "\n",
    "upload_experiment_data_percentage = len(upload_experiment_data_papers) / len(available_papers) * 100\n",
    "\n",
    "print(len(upload_experiment_data_papers),upload_experiment_data_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(upload_experiment_data_papers[upload_experiment_data_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "upload_experiment_data_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: what type of experiments?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFj6rttJB0Bn"
   },
   "outputs": [],
   "source": [
    "# paper year not mentioned.\n",
    "available_papers[available_papers[\"Paper Year\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2mIdvT54oE1"
   },
   "outputs": [],
   "source": [
    "# available_papers[\"Paper Year\"] = available_papers[\"Paper Year\"].astype(int)\n",
    "\n",
    "available_papers[\"Datasets Used\"] = available_papers[\"Does the paper use datasets from OpenML?\"] == \"yes\"\n",
    "available_papers[\"Benchmark Used\"] = available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"yes\"\n",
    "\n",
    "# Group data by year for visualization\n",
    "datasets_by_year = available_papers.groupby(\"Paper Year\")[\"Datasets Used\"].sum()\n",
    "benchmarks_by_year = available_papers.groupby(\"Paper Year\")[\"Benchmark Used\"].sum()\n",
    "\n",
    "# Filter out the year 2025\n",
    "datasets_by_year_filtered = datasets_by_year[datasets_by_year.index != 2025]\n",
    "benchmarks_by_year_filtered = benchmarks_by_year[benchmarks_by_year.index != 2025]\n",
    "\n",
    "# Font sizes\n",
    "LABEL_SIZE, TITLE_SIZE, NUMBER_SIZE = 14, 20, 14\n",
    "\n",
    "def add_data_labels(ax, years, values, color):\n",
    "    \"\"\"Helper function to add data labels with consistent formatting\"\"\"\n",
    "    for year, value in zip(years, values):\n",
    "        if year >= 2023:\n",
    "            va_alignment = 'top'\n",
    "            y_offset, x_offset = -7, 0.3\n",
    "        else:\n",
    "            va_alignment = 'bottom'\n",
    "            y_offset, x_offset = 0, 0\n",
    "\n",
    "        ax.text(year + x_offset, value + y_offset, str(value),\n",
    "                color=color, fontsize=NUMBER_SIZE,\n",
    "                ha=\"right\", va=va_alignment)\n",
    "\n",
    "# Create figure and set padding\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "y_max = max(datasets_by_year_filtered.max(), benchmarks_by_year_filtered.max())\n",
    "padding_factor = 1.05\n",
    "ax.set_ylim(-(y_max * padding_factor - y_max), y_max * padding_factor)\n",
    "\n",
    "# Define and add annotations\n",
    "annotations = {\n",
    "    2016: {\"text\": \"OpenML R\\nPackage on\\nCRAN\", \"y_pos\": 50},\n",
    "    2017: {\"text\": \"Preprint of OpenML\\nBenchmarking Suites\\nPaper, OpenML R Paper\", \"y_pos\": 100},\n",
    "    2018: {\"text\": \"openml-python\\non PyPI\", \"y_pos\": 150},\n",
    "    2019: {\"text\": \"First AutoML\\nBenchmark Paper\", \"y_pos\": 175},\n",
    "    2021: {\"text\": \"NeurIPS Paper on OpenML\\nBenchmarking Suites, OpenML\\nPython Paper\", \"y_pos\": 50},\n",
    "    2023: {\"text\": \"mlr3oml R Package\\non CRAN\", \"y_pos\": 100}\n",
    "}\n",
    "\n",
    "for year, anno_dict in annotations.items():\n",
    "    y_pos = anno_dict[\"y_pos\"]\n",
    "    ax.vlines(x=year, ymin=ax.get_ylim()[0], ymax=y_pos, colors='black', alpha=.3)\n",
    "    ax.hlines(y=y_pos, xmin=year-0.1, xmax=year+0.1, colors='black', alpha=.3)\n",
    "    ax.text(year, y_pos + 5, anno_dict[\"text\"],\n",
    "            ha='center', va='bottom', fontsize=12,\n",
    "            color='black', alpha=.6)\n",
    "\n",
    "# Plot lines and add data labels\n",
    "for data, color, label in [\n",
    "    (datasets_by_year_filtered, 'blue', \"Papers Using OpenML Datasets\"),\n",
    "    (benchmarks_by_year_filtered, 'red', \"Papers Using OpenML Benchmarking Suites\")\n",
    "]:\n",
    "    ax.plot(data.index, data.values, color=color, marker=\"o\", label=label)\n",
    "    add_data_labels(ax, data.index, data.values, color)\n",
    "\n",
    "# Style the axes\n",
    "ax.set_xticks(range(min(datasets_by_year.index), max(datasets_by_year.index)))\n",
    "ax.set_xticklabels(range(min(datasets_by_year.index), max(datasets_by_year.index)),\n",
    "                   rotation=-45)\n",
    "ax.tick_params(axis='both', which='major', labelsize=LABEL_SIZE, width=1.5)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add labels and grid\n",
    "ax.set_xlabel(\"Year\", fontsize=18)\n",
    "ax.set_ylabel(\"Number of Papers\", fontsize=18)\n",
    "ax.set_title(\"Datasets and Benchmarks Used by Papers per Year\", fontsize=TITLE_SIZE)\n",
    "ax.legend(fontsize=14)\n",
    "#ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "with PdfPages(\"Datasets_and_Benchmark_per_year.pdf\") as fh:\n",
    "    fh.savefig(fig, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzMytxQAPYdR"
   },
   "outputs": [],
   "source": [
    "other_interactions = available_papers[available_papers[\"Does the paper interact in any other way with OpenML?\"] == \"yes\"]\n",
    "\n",
    "other_interactions_percentage = len(other_interactions) / len(available_papers) * 100\n",
    "\n",
    "print(len(other_interactions) , other_interactions_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(other_interactions[other_interactions[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "\n",
    "other_interactions[[\"Does the paper interact in any other way with OpenML?\", \"if yes: short (e.g., 1 sentence) explanation how?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLFvS-TJRLpW"
   },
   "outputs": [],
   "source": [
    "# Unique papers which are starred for special mention\n",
    "starred_papers = available_papers[available_papers[\"Star it as some cool project to be showcased in our paper?\"] == \"yes\"]\n",
    "\n",
    "print(len(starred_papers))\n",
    "\n",
    "starred_papers[[\"Paper ID (from shared sheet)\", \"If yes, please motivate your answer\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPGMHf91yR4h"
   },
   "outputs": [],
   "source": [
    "# output_file = \"starred_papers.csv\"\n",
    "# starred_papers.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuPbcG5FRk0U"
   },
   "outputs": [],
   "source": [
    "thesis_papers = available_papers[available_papers[\"Is the paper a thesis (Bachelor's, Master's, or PhD)?\"] == \"yes\"]\n",
    "\n",
    "# Also, check for any occurrence of the word 'thesis' in the \"Optional short description\"\n",
    "thesis_keyword_papers = available_papers[available_papers[\"Optional short description\"].str.contains(\"thesis\", case=False, na=False)]\n",
    "all_thesis_papers = pd.concat([thesis_papers, thesis_keyword_papers]).drop_duplicates()\n",
    "\n",
    "\n",
    "thesis_percentage = len(all_thesis_papers) / len(available_papers) * 100\n",
    "print(\"Number of thesis supervised by core-authors: \",len(all_thesis_papers[all_thesis_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"yes\"]))\n",
    "\n",
    "print(len(all_thesis_papers), thesis_percentage)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
