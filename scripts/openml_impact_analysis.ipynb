{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCCl0TllgC4A"
   },
   "source": [
    "# OpenML Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO5_qVrXgC4C"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --quiet\n",
    "!pip install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "519xCC91_sMH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz5J5Z7bVhhv"
   },
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAzne5nHARM6"
   },
   "outputs": [],
   "source": [
    "path1 = '../data/collected_papers.csv' # Original list of collected papers\n",
    "path2 = \"../data/Final_survey_data.csv\" # Survey Results\n",
    "\n",
    "df = pd.read_csv(path2)\n",
    "\n",
    "print(\"Total no of reviews recieved: \", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYZ9Bi1qEWqk"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhQQfvvt9kH8"
   },
   "source": [
    "### Sanity Check:\n",
    "1. Remove duplicates\n",
    "2. Match paper ID and paper title from original list\n",
    "3. Remove papers with wrong year (<2014)\n",
    "4. Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4xo6Knlovwe"
   },
   "outputs": [],
   "source": [
    "Total_papers = pd.read_csv(path1) # Original list of scraped paper\n",
    "print(\"Total scraped papers: \", len(Total_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s0As6Lq9jp3"
   },
   "outputs": [],
   "source": [
    "assigned_papers_dict = Total_papers[[\"Paper ID\", \"Title\"]].set_index('Paper ID').to_dict()\n",
    "assigned_papers_dict = assigned_papers_dict[\"Title\"]\n",
    "assigned_papers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xod_d_tRgC4F"
   },
   "outputs": [],
   "source": [
    "sheet1 = Total_papers[[\"Paper ID\", \"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]]\n",
    "sheet1.rename(columns={\"Paper ID\": \"Paper ID (from shared sheet)\"}, inplace=True)\n",
    "df = df.merge(sheet1, on=\"Paper ID (from shared sheet)\", how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayYUeZPFz3Dd"
   },
   "outputs": [],
   "source": [
    "# Step 1: Remove Dublicates\n",
    "\n",
    "# Duplicate entries\n",
    "duplicate_rows = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
    "\n",
    "print(len(duplicate_rows))\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3tKQM5f0AGk"
   },
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
    "# Save duplicates to a separate DataFrame before removal\n",
    "duplicates_removed = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")]\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")\n",
    "\n",
    "print(len(duplicates_removed))\n",
    "duplicates_removed\n",
    "# ToDo - better strategy to remove duplicates - ask reviewers to correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88YrvITVAKTO"
   },
   "outputs": [],
   "source": [
    "print(\"No of reviews after dublicate removal: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YWfRQuIsSl_l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Remove mismatches\n",
    "\n",
    "mismatches = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        paper_id = row[\"Paper ID (from shared sheet)\"]\n",
    "        paper_title = row[\"Paper Title\"].strip().lower()\n",
    "        assigned_title = assigned_papers_dict.get(paper_id).strip().lower()\n",
    "\n",
    "        if paper_title not in assigned_title:\n",
    "            mismatches.append({\n",
    "                \"Paper ID\": paper_id,\n",
    "                \"Given Title\": row[\"Paper Title\"],\n",
    "                \"Expected Title\": assigned_papers_dict.get(paper_id, \"Not Found\")\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        mismatches.append({\n",
    "            \"Paper ID\": row[\"Paper ID (from shared sheet)\"],\n",
    "            \"Error\": str(e),\n",
    "            \"Given Title\": row.get(\"Paper Title\", \"Not Found\")\n",
    "        })\n",
    "        continue\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "mismatches_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB7cPdcY2EKD"
   },
   "outputs": [],
   "source": [
    "# Mismatched paper ID and title - 901, 240, 881, 1467, 1472, 108 - to be removed\n",
    "paper_ids_to_remove = [901, 240, 881, 1467, 1472, 108] #contact authors\n",
    "\n",
    "df = df[~df[\"Paper ID (from shared sheet)\"].isin(paper_ids_to_remove)]\n",
    "df[df[\"Paper ID (from shared sheet)\"] == 108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJamFvhNzDEU"
   },
   "outputs": [],
   "source": [
    "print(\"No of reviews after duplicate removal: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m49r76Sr3EB-"
   },
   "outputs": [],
   "source": [
    "# Drop rows with all NaN or NaT values\n",
    "df = df.dropna(how=\"all\")\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmSIfxId3_c2"
   },
   "outputs": [],
   "source": [
    "# Remove papers with a year before 2014 - temporary messure. some incorrect paper year.\n",
    "\n",
    "# df[\"Paper Year\"] = df[\"Paper Year\"].astype(int)\n",
    "\n",
    "# # Check the updated column type\n",
    "# print(df[\"Paper Year\"].dtypes)\n",
    "\n",
    "df = df[df[\"Paper Year\"] >= 2014]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN3bacOo9cfg"
   },
   "outputs": [],
   "source": [
    "print(\"Paper with wrong year: \", 1696-1665)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1M1RFW5P0q1R"
   },
   "outputs": [],
   "source": [
    "count_yes = df[\"Paper Available\"].value_counts().get(\"Yes\", 0)\n",
    "print(count_yes)\n",
    "print(len(df)-count_yes + (len(Total_papers)-1702))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0PWp6MJ1jYJ"
   },
   "outputs": [],
   "source": [
    "count_yes = df[\"Paper in English\"].value_counts().get(\"Yes\", 0)\n",
    "print(count_yes)\n",
    "print(len(df)-count_yes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8ZTz_WkgC4I"
   },
   "outputs": [],
   "source": [
    "# No fo paper which are both not fully available and not in english\n",
    "overlap = df[(df[\"Paper Available\"] != \"Yes\") & (df[\"Paper in English\"] != \"Yes\")]\n",
    "len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZbAhJSve9-0"
   },
   "outputs": [],
   "source": [
    "\n",
    "columns_to_analyze = [\"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]\n",
    "\n",
    "# Calculate the percentage of True values for each column\n",
    "percentages = Total_papers[columns_to_analyze].mean() * 100\n",
    "\n",
    "# Print the results\n",
    "for column, percentage in percentages.items():\n",
    "    print(f\"Percentage of papers with True for {column}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xbi0A9kP628A"
   },
   "outputs": [],
   "source": [
    "# Filter rows not in final_df\n",
    "not_in_final_df = df[~((df[\"Paper Available\"] == \"Yes\") &  (df[\"Paper in English\"] == \"Yes\"))]\n",
    "\n",
    "print(len(not_in_final_df))\n",
    "# Display the rows not in final_df\n",
    "not_in_final_df[[\"Paper Available\", \"Paper in English\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhEJ8Azj9750"
   },
   "outputs": [],
   "source": [
    "# Should only consider paper available + in english\n",
    "available_papers = df[(df[\"Paper Available\"] == \"Yes\") & (df[\"Paper in English\"] == \"Yes\")].copy()\n",
    "print(\"Final number of papers for analysis: \", len(available_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq_ceWQpgC4J"
   },
   "source": [
    "Final statistics\n",
    "\n",
    "1. Total exctracted paper: 1719\n",
    "2. Paper used in analysis: 1482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B30J-apAgC4J"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-9378NBGGRu"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of papers per year\n",
    "papers_per_year = available_papers[\"Paper Year\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(papers_per_year.index, papers_per_year.values, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.title(\"Number of OpenML Cited Papers Per Year\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(linestyle='--', alpha=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCdfjAQJYFR7"
   },
   "outputs": [],
   "source": [
    "# What percentage of the papers does just cite OpenML, but not actively interact with it?\n",
    "\n",
    "interaction_columns = [\n",
    "    \"Does the paper use datasets from OpenML?\",\n",
    "    \"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\",\n",
    "    \"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \",\n",
    "    \"Does the paper upload datasets to OpenML?\",\n",
    "    \"Does the paper upload experiment data to OpenML?\",\n",
    "    \"Does the paper interact in any other way with OpenML?\"\n",
    "]\n",
    "\n",
    "just_citing_papers = available_papers[(available_papers[interaction_columns] == \"No\").all(axis=1)]\n",
    "\n",
    "percentage_citing_only = (len(just_citing_papers) / len(available_papers)) * 100\n",
    "print(len(just_citing_papers), round(percentage_citing_only, 2))\n",
    "\n",
    "core_citing_papers = just_citing_papers[just_citing_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]\n",
    "print(len(core_citing_papers), round((len(core_citing_papers)/len(just_citing_papers))*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdQJYPjIfZIW"
   },
   "outputs": [],
   "source": [
    "# out of above papers which just cites openml, % of paper citing Openml-2014 paper\n",
    "\n",
    "just_citing_openml_2014 = just_citing_papers[just_citing_papers[\"openml-2014\"] == True]\n",
    "print(len(just_citing_openml_2014), (len(just_citing_openml_2014) / len(just_citing_papers)) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi5-nmvVHPVg"
   },
   "outputs": [],
   "source": [
    "english_counts = df[\"Paper in English\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(english_counts, labels=english_counts.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Papers in English\")\n",
    "\n",
    "\n",
    "core_member_counts = available_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"].value_counts()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(core_member_counts, labels=core_member_counts.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Papers with OpenML Core Member Co-author\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3xB5ML_IMPu"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "dataset_papers = available_papers[available_papers[\"Does the paper use datasets from OpenML?\"] == \"Yes\"]\n",
    "dataset_percentage = (len(dataset_papers) / len(available_papers)) * 100\n",
    "print(dataset_percentage, (len(dataset_papers)))\n",
    "print(\"Number of paper by core-authors: \",len(dataset_papers[dataset_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "unclear_dataset_papers = available_papers.dropna(subset=[\"If unclear, one sentence explanation?\"])\n",
    "unclear_dataset_papers = unclear_dataset_papers[[\"Does the paper use datasets from OpenML?\", \"If unclear, one sentence explanation?\"]]\n",
    "unclear_dataset_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4VmZNdkKJAC"
   },
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "benchmark_papers = available_papers[available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"Yes\"]\n",
    "benchmark_percentage = (len(benchmark_papers) / len(available_papers[available_papers[\"Paper Year\"] >= 2017])) * 100\n",
    "print(benchmark_percentage, len(benchmark_papers))\n",
    "\n",
    "benchmark_datasets = benchmark_papers[\"If yes, which benchmark suites?\"]\n",
    "\n",
    "print(\"Number of paper by core-authors: \",len(benchmark_papers[benchmark_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "# Print list of benchmark datasets\n",
    "unique_benchmark_datasets = (\n",
    "    benchmark_datasets.str.lower().str.strip().unique()\n",
    ")\n",
    "\n",
    "\n",
    "unique_benchmark_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWw_HzTJKbs5"
   },
   "outputs": [],
   "source": [
    "#  Experiment data (Runs)\n",
    "\n",
    "experiment_papers = available_papers[available_papers[\"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \"] == \"Yes\"]\n",
    "experiment_percentage = (len(experiment_papers) / len(available_papers)) * 100\n",
    "\n",
    "print(len(experiment_papers) ,experiment_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(experiment_papers[experiment_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "\n",
    "experiment_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: how does it use this data?\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV6nngGINXF7"
   },
   "outputs": [],
   "source": [
    "upload_datasets = available_papers[available_papers[\"Does the paper upload datasets to OpenML?\"] == \"Yes\"]\n",
    "\n",
    "upload_datasets_percentage = len(upload_datasets) / len(available_papers) * 100\n",
    "\n",
    "print( len(upload_datasets), upload_datasets_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(upload_datasets[upload_datasets[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "\n",
    "upload_datasets[[\"Paper ID (from shared sheet)\", \"Does the paper upload datasets to OpenML?\", \"If yes, which dataset?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCvmqWoROv6y"
   },
   "outputs": [],
   "source": [
    "upload_experiment_data_papers = available_papers[available_papers[\"Does the paper upload experiment data to OpenML?\"] == \"Yes\"]\n",
    "\n",
    "upload_experiment_data_percentage = len(upload_experiment_data_papers) / len(available_papers) * 100\n",
    "\n",
    "print(len(upload_experiment_data_papers),upload_experiment_data_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(upload_experiment_data_papers[upload_experiment_data_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "upload_experiment_data_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: what type of experiments?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFj6rttJB0Bn"
   },
   "outputs": [],
   "source": [
    "# paper year not mentioned.\n",
    "available_papers[available_papers[\"Paper Year\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2mIdvT54oE1"
   },
   "outputs": [],
   "source": [
    "available_papers[\"Paper Year\"] = available_papers[\"Paper Year\"].astype(int)\n",
    "\n",
    "available_papers[\"Datasets Used\"] = available_papers[\"Does the paper use datasets from OpenML?\"] == \"Yes\"\n",
    "available_papers[\"Benchmark Used\"] = available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"Yes\"\n",
    "\n",
    "# Group data by year for visualization\n",
    "datasets_by_year = available_papers.groupby(\"Paper Year\")[\"Datasets Used\"].sum()\n",
    "benchmarks_by_year = available_papers.groupby(\"Paper Year\")[\"Benchmark Used\"].sum()\n",
    "\n",
    "# Filter out the year 2025\n",
    "datasets_by_year_filtered = datasets_by_year[datasets_by_year.index != 2025]\n",
    "benchmarks_by_year_filtered = benchmarks_by_year[benchmarks_by_year.index != 2025]\n",
    "\n",
    "# Font sizes\n",
    "LABEL_SIZE, TITLE_SIZE, NUMBER_SIZE = 14, 20, 14\n",
    "\n",
    "def add_data_labels(ax, years, values, color):\n",
    "    \"\"\"Helper function to add data labels with consistent formatting\"\"\"\n",
    "    for year, value in zip(years, values):\n",
    "        if year >= 2023:\n",
    "            va_alignment = 'top'\n",
    "            y_offset, x_offset = -7, 0.3\n",
    "        else:\n",
    "            va_alignment = 'bottom'\n",
    "            y_offset, x_offset = 0, 0\n",
    "\n",
    "        ax.text(year + x_offset, value + y_offset, str(value),\n",
    "                color=color, fontsize=NUMBER_SIZE,\n",
    "                ha=\"right\", va=va_alignment)\n",
    "\n",
    "# Create figure and set padding\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "y_max = max(datasets_by_year_filtered.max(), benchmarks_by_year_filtered.max())\n",
    "padding_factor = 1.05\n",
    "ax.set_ylim(-(y_max * padding_factor - y_max), y_max * padding_factor)\n",
    "\n",
    "# Define and add annotations\n",
    "annotations = {\n",
    "    2016: {\"text\": \"OpenML R\\nPackage on\\nCRAN\", \"y_pos\": 50},\n",
    "    2017: {\"text\": \"Preprint of OpenML\\nBenchmarking Suites\\nPaper, OpenML R Paper\", \"y_pos\": 100},\n",
    "    2018: {\"text\": \"openml-python\\non PyPI\", \"y_pos\": 150},\n",
    "    2019: {\"text\": \"First AutoML\\nBenchmark Paper\", \"y_pos\": 175},\n",
    "    2021: {\"text\": \"NeurIPS Paper on OpenML\\nBenchmarking Suites, OpenML\\nPython Paper\", \"y_pos\": 50},\n",
    "    2023: {\"text\": \"mlr3oml R Package\\non CRAN\", \"y_pos\": 100}\n",
    "}\n",
    "\n",
    "for year, anno_dict in annotations.items():\n",
    "    y_pos = anno_dict[\"y_pos\"]\n",
    "    ax.vlines(x=year, ymin=ax.get_ylim()[0], ymax=y_pos, colors='black', alpha=.3)\n",
    "    ax.hlines(y=y_pos, xmin=year-0.1, xmax=year+0.1, colors='black', alpha=.3)\n",
    "    ax.text(year, y_pos + 5, anno_dict[\"text\"],\n",
    "            ha='center', va='bottom', fontsize=12,\n",
    "            color='black', alpha=.6)\n",
    "\n",
    "# Plot lines and add data labels\n",
    "for data, color, label in [\n",
    "    (datasets_by_year_filtered, 'blue', \"Papers Using OpenML Datasets\"),\n",
    "    (benchmarks_by_year_filtered, 'red', \"Papers Using OpenML Benchmarking Suites\")\n",
    "]:\n",
    "    ax.plot(data.index, data.values, color=color, marker=\"o\", label=label)\n",
    "    add_data_labels(ax, data.index, data.values, color)\n",
    "\n",
    "# Style the axes\n",
    "ax.set_xticks(range(min(datasets_by_year.index), max(datasets_by_year.index)))\n",
    "ax.set_xticklabels(range(min(datasets_by_year.index), max(datasets_by_year.index)),\n",
    "                   rotation=-45)\n",
    "ax.tick_params(axis='both', which='major', labelsize=LABEL_SIZE, width=1.5)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add labels and grid\n",
    "ax.set_xlabel(\"Year\", fontsize=18)\n",
    "ax.set_ylabel(\"Number of Papers\", fontsize=18)\n",
    "ax.set_title(\"Datasets and Benchmarks Used by Papers per Year\", fontsize=TITLE_SIZE)\n",
    "ax.legend(fontsize=14)\n",
    "#ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "with PdfPages(\"Datasets_and_Benchmark_per_year.pdf\") as fh:\n",
    "    fh.savefig(fig, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzMytxQAPYdR"
   },
   "outputs": [],
   "source": [
    "other_interactions = available_papers[available_papers[\"Does the paper interact in any other way with OpenML?\"] == \"Yes\"]\n",
    "\n",
    "other_interactions_percentage = len(other_interactions) / len(available_papers) * 100\n",
    "\n",
    "print(len(other_interactions) , other_interactions_percentage)\n",
    "print(\"Number of paper by core-authors: \",len(other_interactions[other_interactions[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "\n",
    "other_interactions[[\"Does the paper interact in any other way with OpenML?\", \"if yes: short (e.g., 1 sentence) explanation how?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLFvS-TJRLpW"
   },
   "outputs": [],
   "source": [
    "starred_papers = available_papers[available_papers[\"Star it as some cool project to be showcased in our paper?\"] == \"Yes\"]\n",
    "\n",
    "print(len(starred_papers))\n",
    "\n",
    "starred_papers[[\"Paper ID (from shared sheet)\", \"If yes, please motivate your answer\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPGMHf91yR4h"
   },
   "outputs": [],
   "source": [
    "# output_file = \"starred_papers.csv\"\n",
    "# starred_papers.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuPbcG5FRk0U"
   },
   "outputs": [],
   "source": [
    "thesis_papers = available_papers[available_papers[\"Is the paper a thesis (Bachelor's, Master's, or PhD)?\"] == \"Yes\"]\n",
    "\n",
    "# Also, check for any occurrence of the word 'thesis' in the \"Optional short description\"\n",
    "thesis_keyword_papers = available_papers[available_papers[\"Optional short description\"].str.contains(\"thesis\", case=False, na=False)]\n",
    "all_thesis_papers = pd.concat([thesis_papers, thesis_keyword_papers]).drop_duplicates()\n",
    "\n",
    "\n",
    "thesis_percentage = len(all_thesis_papers) / len(available_papers) * 100\n",
    "print(\"Number of paper by core-authors: \",len(all_thesis_papers[all_thesis_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
    "\n",
    "print(len(all_thesis_papers), thesis_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEW3aN-mS-WO"
   },
   "outputs": [],
   "source": [
    "# Differentiate in skill level between core members and non-core members\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
