# Methodology

For our literature analysis, co-authors reviewed the papers collaboratively (at least 100 papers each) and were asked to answer the following questions for each paper:

- **What is the paper title?**
- **What is the year of publication?**
- **Is the paper available?**
- **Is the paper in English?**
- **Is an OpenML core contributor co-author? If so, who?**
- **Does the paper use datasets from OpenML?**
- **Does the paper use benchmarking suites? If so, which?**
- **Does the paper use experiment data from OpenML (e.g., *runs*)? If yes, how?**
- **Does the paper upload datasets to OpenML? If yes, which?**
- **Does the paper upload experiment data to OpenML? If yes, what type of experiment data?**
- **Does the paper interact with OpenML in any other way? How?**
- **Should the paper be considered to be highlighted? If so, why?**
- **Is the paper a thesis?**
- **(Optional) Provide a short description or remark.**

The first questions were mainly designed to ensure the answers correspond to the expected paper and serve as a sanity check. In rare cases, citing OpenML only happens in some iterations of the paper, and answers were expected to be given with respect to that particular version of the paper.

The review process yielded the following results:

- **Total papers initially identified**: 1789
- **Papers not fully available**: 184
- **Papers not in English and without available translations**: 72  
   (*22 papers were both unavailable and not in English.*)
- **Papers published after 2025**: 24
(*Note: We exclude papers published in 2025 as the year is still in progress, to avoid skewed interpretations of trends.*)
- **Final papers analyzed**: 1528
